import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from wordcloud import WordCloud
import re
import jieba
import emoji
from collections import Counter
import networkx as nx
from datetime import datetime
import numpy as np
from textblob import TextBlob
import warnings
warnings.filterwarnings('ignore')

# ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from fonts import get_japanese_font_path, get_font_family

# Matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
font_path = get_japanese_font_path()
if font_path:
    # ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’Matplotlibã«ç™»éŒ²
    font_prop = fm.FontProperties(fname=font_path)
    plt.rcParams['font.family'] = font_prop.get_name()
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
    plt.rcParams['font.sans-serif'] = ['Noto Sans JP', 'DejaVu Sans', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="XæŠ•ç¨¿åˆ†æã‚¢ãƒ—ãƒª",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("ğŸ“Š XæŠ•ç¨¿åˆ†æã‚¢ãƒ—ãƒª")
st.markdown("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€Xã®æŠ•ç¨¿ã‚’åˆ†æã—ã¾ã—ã‚‡ã†ï¼")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
st.sidebar.header("ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
uploaded_file = st.sidebar.file_uploader(
    "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
    type=['csv'],
    help="æŠ•ç¨¿æ—¥æ™‚ã¨ãƒ†ã‚­ã‚¹ãƒˆã®åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
)

# ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ¤œç´¢
st.sidebar.header("ğŸ” ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ¤œç´¢")
search_hashtag = st.sidebar.text_input(
    "åˆ†æã—ãŸã„ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’å…¥åŠ›",
    placeholder="#ãƒãƒ³ãƒ—ãƒ­ç ”",
    help="ç‰¹å®šã®ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã«é–¢ã™ã‚‹æŠ•ç¨¿ã®ã¿ã‚’åˆ†æã—ã¾ã™"
)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
@st.cache_data
def load_and_process_data(file):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§å‰å‡¦ç†ã‚’è¡Œã†"""
    try:
        df = pd.read_csv(file)
        
        # åˆ—åã®æ­£è¦åŒ–
        df.columns = df.columns.str.strip()
        
        # æŠ•ç¨¿æ—¥æ™‚ã®å‡¦ç†
        if 'æŠ•ç¨¿æ—¥æ™‚' in df.columns:
            df['æŠ•ç¨¿æ—¥æ™‚'] = pd.to_datetime(df['æŠ•ç¨¿æ—¥æ™‚'], format='%Y/%m/%d %H:%M', errors='coerce')
        elif 'created_at' in df.columns:
            df['æŠ•ç¨¿æ—¥æ™‚'] = pd.to_datetime(df['created_at'], errors='coerce')
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã®ç¢ºèª
        text_column = None
        for col in ['ãƒ†ã‚­ã‚¹ãƒˆ', 'text', 'content', 'tweet_text']:
            if col in df.columns:
                text_column = col
                break
        
        if text_column is None:
            st.error("ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚CSVãƒ•ã‚¡ã‚¤ãƒ«ã«'ãƒ†ã‚­ã‚¹ãƒˆ'åˆ—ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        df['ãƒ†ã‚­ã‚¹ãƒˆ_å‡¦ç†æ¸ˆã¿'] = df[text_column].astype(str).apply(preprocess_text)
        
        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®æŠ½å‡º
        df['ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°'] = df[text_column].astype(str).apply(extract_hashtags)
        
        # æ„Ÿæƒ…åˆ†æ
        df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢'] = df[text_column].astype(str).apply(analyze_sentiment)
        
        return df, text_column
        
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None, None

def preprocess_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    if pd.isna(text):
        return ""
    
    # URLã®é™¤å»
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', str(text))
    
    # ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã®é™¤å»
    text = re.sub(r'@\w+', '', text)
    
    # çµµæ–‡å­—ã®é™¤å»
    text = emoji.replace_emoji(text, replace='')
    
    # ç‰¹æ®Šæ–‡å­—ã®é™¤å»
    text = re.sub(r'[^\w\s#]', ' ', text)
    
    # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã®ç©ºç™½ã«
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def extract_hashtags(text):
    """ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’æŠ½å‡º"""
    if pd.isna(text):
        return []
    hashtags = re.findall(r'#\w+', str(text))
    return hashtags

def analyze_sentiment(text):
    """æ„Ÿæƒ…åˆ†æï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    if pd.isna(text) or text == "":
        return 0
    
    try:
        blob = TextBlob(text)
        return blob.sentiment.polarity
    except:
        return 0

def filter_by_hashtag(df, hashtag):
    """ç‰¹å®šã®ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    if not hashtag or hashtag == "":
        return df
    
    hashtag = hashtag.lower()
    filtered_df = df[df['ãƒ†ã‚­ã‚¹ãƒˆ_å‡¦ç†æ¸ˆã¿'].str.contains(hashtag, case=False, na=False)]
    
    if len(filtered_df) == 0:
        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°åˆ—ã§æ¤œç´¢
        filtered_df = df[df['ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°'].apply(lambda x: any(hashtag in tag.lower() for tag in x))]
    
    return filtered_df

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
if uploaded_file is not None:
    df, text_column = load_and_process_data(uploaded_file)
    
    if df is not None:
        st.success(f"âœ… ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼ ({len(df)}ä»¶ã®æŠ•ç¨¿)")
        
        # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if search_hashtag:
            df_filtered = filter_by_hashtag(df, search_hashtag)
            st.info(f"ğŸ” '{search_hashtag}' ã«é–¢ã™ã‚‹æŠ•ç¨¿: {len(df_filtered)}ä»¶")
        else:
            df_filtered = df
            st.info("ğŸ” å…¨æŠ•ç¨¿ã‚’åˆ†æå¯¾è±¡ã¨ã—ã¦ã„ã¾ã™")
        
        if len(df_filtered) > 0:
            # ã‚¿ãƒ–ã‚’ä½œæˆ
            tab1, tab2, tab3, tab4, tab5 = st.tabs([
                "ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ", 
                "ğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°", 
                "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", 
                "ğŸ—ºï¸ å˜èªãƒãƒƒãƒ—", 
                "ğŸ“… æ™‚ç³»åˆ—åˆ†æ"
            ])
            
            with tab1:
                st.header("ğŸ“Š åŸºæœ¬çµ±è¨ˆ")
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("ç·æŠ•ç¨¿æ•°", len(df_filtered))
                
                with col2:
                    avg_length = df_filtered['ãƒ†ã‚­ã‚¹ãƒˆ_å‡¦ç†æ¸ˆã¿'].str.len().mean()
                    st.metric("å¹³å‡æ–‡å­—æ•°", f"{avg_length:.1f}")
                
                with col3:
                    total_hashtags = sum(len(hashtags) for hashtags in df_filtered['ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°'])
                    st.metric("ç·ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ•°", total_hashtags)
                
                with col4:
                    avg_sentiment = df_filtered['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢'].mean()
                    st.metric("å¹³å‡æ„Ÿæƒ…ã‚¹ã‚³ã‚¢", f"{avg_sentiment:.3f}")
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                st.dataframe(
                    df_filtered[['æŠ•ç¨¿æ—¥æ™‚', 'ãƒ†ã‚­ã‚¹ãƒˆ_å‡¦ç†æ¸ˆã¿', 'ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°', 'æ„Ÿæƒ…ã‚¹ã‚³ã‚¢']].head(10),
                    use_container_width=True
                )
            
            with tab2:
                st.header("ğŸ† ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æ")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("ğŸ“Š ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                    all_hashtags = []
                    for hashtags in df_filtered['ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°']:
                        all_hashtags.extend(hashtags)
                    
                    hashtag_counts = Counter(all_hashtags)
                    top_hashtags = dict(hashtag_counts.most_common(10))
                    
                    if top_hashtags:
                        fig = px.bar(
                            x=list(top_hashtags.values()),
                            y=list(top_hashtags.keys()),
                            orientation='h',
                            title="ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ä½¿ç”¨é »åº¦ãƒˆãƒƒãƒ—10"
                        )
                        fig.update_layout(height=400)
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
                with col2:
                    st.subheader("ğŸ“Š æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                    sentiment_df = df_filtered.nlargest(10, 'æ„Ÿæƒ…ã‚¹ã‚³ã‚¢')[['ãƒ†ã‚­ã‚¹ãƒˆ_å‡¦ç†æ¸ˆã¿', 'æ„Ÿæƒ…ã‚¹ã‚³ã‚¢']]
                    
                    if not sentiment_df.empty:
                        fig = px.bar(
                            x=sentiment_df['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢'],
                            y=sentiment_df['ãƒ†ã‚­ã‚¹ãƒˆ_å‡¦ç†æ¸ˆã¿'].str[:30] + "...",
                            orientation='h',
                            title="æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ä¸Šä½10ä»¶"
                        )
                        fig.update_layout(height=400)
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.info("æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
            with tab3:
                st.header("â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                
                # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                all_text = ' '.join(df_filtered['ãƒ†ã‚­ã‚¹ãƒˆ_å‡¦ç†æ¸ˆã¿'].dropna())
                
                if all_text.strip():
                    # æ—¥æœ¬èªã®åˆ†ã‹ã¡æ›¸ã
                    words = jieba.cut(all_text)
                    word_text = ' '.join(words)
                    
                    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
                    font_path = get_japanese_font_path()
                    
                    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆ
                    wordcloud = WordCloud(
                        width=800,
                        height=400,
                        background_color='white',
                        max_words=100,
                        colormap='viridis',
                        font_path=font_path if font_path else None
                    ).generate(word_text)
                    
                    # è¡¨ç¤º
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.imshow(wordcloud, interpolation='bilinear')
                    ax.axis('off')
                    st.pyplot(fig)
                    
                    # å˜èªé »åº¦ã®æ£’ã‚°ãƒ©ãƒ•
                    word_counts = Counter(word_text.split())
                    top_words = dict(word_counts.most_common(20))
                    
                    fig = px.bar(
                        x=list(top_words.values()),
                        y=list(top_words.keys()),
                        orientation='h',
                        title="å˜èªå‡ºç¾é »åº¦ãƒˆãƒƒãƒ—20"
                    )
                    fig.update_layout(height=400)
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            
            with tab4:
                st.header("ğŸ—ºï¸ å˜èªãƒãƒƒãƒ—ï¼ˆå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰")
                
                # å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä½œæˆ
                if len(df_filtered) > 0:
                    # å˜èªã®å…±èµ·ã‚’è¨ˆç®—
                    word_pairs = []
                    for text in df_filtered['ãƒ†ã‚­ã‚¹ãƒˆ_å‡¦ç†æ¸ˆã¿'].dropna():
                        words = jieba.cut(text)
                        words = [w for w in words if len(w) > 1]  # 1æ–‡å­—ã®å˜èªã‚’é™¤å¤–
                        for i in range(len(words)):
                            for j in range(i+1, min(i+3, len(words))):  # éš£æ¥ã™ã‚‹å˜èªã®ã¿
                                word_pairs.append(tuple(sorted([words[i], words[j]])))
                    
                    if word_pairs:
                        pair_counts = Counter(word_pairs)
                        top_pairs = pair_counts.most_common(20)
                        
                        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã®ä½œæˆ
                        G = nx.Graph()
                        for (word1, word2), weight in top_pairs:
                            G.add_edge(word1, word2, weight=weight)
                        
                        # ãƒãƒ¼ãƒ‰ã®ä½ç½®ã‚’è¨ˆç®—
                        pos = nx.spring_layout(G, k=1, iterations=50)
                        
                        # ãƒ—ãƒ­ãƒƒãƒˆ
                        fig, ax = plt.subplots(figsize=(12, 8))
                        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
                        font_path = get_japanese_font_path()
                        if font_path:
                            font_prop = fm.FontProperties(fname=font_path)
                        else:
                            font_prop = fm.FontProperties()
                        
                        nx.draw(
                            G, pos,
                            with_labels=True,
                            node_color='lightblue',
                            node_size=[G.degree(node) * 100 for node in G.nodes()],
                            font_size=8,
                            font_family=font_prop.get_name(),
                            edge_color='gray',
                            width=[G[u][v]['weight'] / 2 for u, v in G.edges()]
                        )
                        plt.title("å˜èªå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", fontsize=14)
                        st.pyplot(fig)
                        
                        # å…±èµ·é »åº¦ã®è¡¨
                        st.subheader("å…±èµ·é »åº¦ä¸Šä½20ä»¶")
                        cooccurrence_df = pd.DataFrame(
                            [(w1, w2, count) for (w1, w2), count in top_pairs],
                            columns=['å˜èª1', 'å˜èª2', 'å…±èµ·å›æ•°']
                        )
                        st.dataframe(cooccurrence_df, use_container_width=True)
                    else:
                        st.info("å…±èµ·ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                else:
                    st.info("ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            
            with tab5:
                st.header("ğŸ“… æ™‚ç³»åˆ—åˆ†æ")
                
                if 'æŠ•ç¨¿æ—¥æ™‚' in df_filtered.columns and not df_filtered['æŠ•ç¨¿æ—¥æ™‚'].isna().all():
                    # æ—¥ä»˜åˆ¥æŠ•ç¨¿æ•°
                    daily_posts = df_filtered.groupby(df_filtered['æŠ•ç¨¿æ—¥æ™‚'].dt.date).size().reset_index()
                    daily_posts.columns = ['æ—¥ä»˜', 'æŠ•ç¨¿æ•°']
                    
                    fig = px.line(
                        daily_posts,
                        x='æ—¥ä»˜',
                        y='æŠ•ç¨¿æ•°',
                        title="æ—¥åˆ¥æŠ•ç¨¿æ•°æ¨ç§»"
                    )
                    fig.update_layout(height=400)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ™‚é–“åˆ¥æŠ•ç¨¿æ•°
                    hourly_posts = df_filtered.groupby(df_filtered['æŠ•ç¨¿æ—¥æ™‚'].dt.hour).size().reset_index()
                    hourly_posts.columns = ['æ™‚é–“', 'æŠ•ç¨¿æ•°']
                    
                    fig = px.bar(
                        hourly_posts,
                        x='æ™‚é–“',
                        y='æŠ•ç¨¿æ•°',
                        title="æ™‚é–“åˆ¥æŠ•ç¨¿æ•°åˆ†å¸ƒ"
                    )
                    fig.update_layout(height=400)
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®æ™‚ç³»åˆ—æ¨ç§»
                    if 'æ„Ÿæƒ…ã‚¹ã‚³ã‚¢' in df_filtered.columns:
                        sentiment_time = df_filtered.groupby(df_filtered['æŠ•ç¨¿æ—¥æ™‚'].dt.date)['æ„Ÿæƒ…ã‚¹ã‚³ã‚¢'].mean().reset_index()
                        sentiment_time.columns = ['æ—¥ä»˜', 'å¹³å‡æ„Ÿæƒ…ã‚¹ã‚³ã‚¢']
                        
                        fig = px.line(
                            sentiment_time,
                            x='æ—¥ä»˜',
                            y='å¹³å‡æ„Ÿæƒ…ã‚¹ã‚³ã‚¢',
                            title="æ—¥åˆ¥å¹³å‡æ„Ÿæƒ…ã‚¹ã‚³ã‚¢æ¨ç§»"
                        )
                        fig.update_layout(height=400)
                        st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("æŠ•ç¨¿æ—¥æ™‚ã®ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        else:
            st.warning("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
else:
    st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®èª¬æ˜
    st.markdown("""
    ### ğŸ“‹ å¯¾å¿œã—ã¦ã„ã‚‹CSVå½¢å¼
    
    ä»¥ä¸‹ã®åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã—ã¦ã„ã¾ã™ï¼š
    - **æŠ•ç¨¿æ—¥æ™‚**: æŠ•ç¨¿ã®æ—¥æ™‚ï¼ˆä¾‹ï¼š2025/08/09 21:27ï¼‰
    - **ãƒ†ã‚­ã‚¹ãƒˆ**: æŠ•ç¨¿ã®å†…å®¹
    
    ### ğŸ” åˆ†ææ©Ÿèƒ½
    
    1. **åŸºæœ¬çµ±è¨ˆ**: æŠ•ç¨¿æ•°ã€å¹³å‡æ–‡å­—æ•°ã€ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æ•°ãªã©
    2. **ãƒ©ãƒ³ã‚­ãƒ³ã‚°**: ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ä½¿ç”¨é »åº¦ã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    3. **ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰**: é »å‡ºå˜èªã®å¯è¦–åŒ–
    4. **å˜èªãƒãƒƒãƒ—**: å˜èªã®å…±èµ·é–¢ä¿‚ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡¨ç¤º
    5. **æ™‚ç³»åˆ—åˆ†æ**: æŠ•ç¨¿æ•°ã®æ¨ç§»ã€æ™‚é–“åˆ†å¸ƒã€æ„Ÿæƒ…ã‚¹ã‚³ã‚¢ã®å¤‰åŒ–
    
    ### ğŸ’¡ ä½¿ã„æ–¹
    
    1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    2. åˆ†æã—ãŸã„ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’å…¥åŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    3. å„ã‚¿ãƒ–ã§åˆ†æçµæœã‚’ç¢ºèª
    """)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("ğŸ“Š XæŠ•ç¨¿åˆ†æã‚¢ãƒ—ãƒª | Streamlit Cloudå¯¾å¿œ")
